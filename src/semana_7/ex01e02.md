# Semana 7

## 1 - Com base no material apresentado no notebook, o que é uma função de ativação (como a ReLU)? Por que normalmente usamos entre as camadas?

Uma função de ativação aparece em cada neurônio logo depois do cálculo da soma ponderada das entradas,ou seja, produto escalar entre entradas e pesos mais o bias. O papel dela é quebrar a linearidade. Se não houvesse função de ativação, mesmo empilhando várias camadas, no fim das contas seria o mesmo que ter uma única camada linear, o que limita muito o que a rede pode aprender.

A ReLU, por exemplo, é uma das mais usadas. Ela simplesmente retorna o valor da entrada se ele for positivo e, se for menor ou igual a zero, retorna zero. Essa simplicidade ajuda no treino e evita alguns problemas de outras ativações.

O ponto central é: essas funções permitem que a rede consiga modelar relações não-lineares entre as features. Sem elas, só daria pra aprender padrões muito básicos, como relações lógicas simples (E, OU). Para problemas mais complexos, como o XOR, as funções de ativação são indispensáveis.


## 2 - Explique o que cada uma das seguintes linhas de código faz e por que ela é necessária:

### a. model.train()

Coloca o modelo no modo de treinamento. Isso significa que algumas camadas passam a se comportar de forma específica para treino, como o dropout (que desliga neurônios aleatoriamente) e a batch normalization (que usa estatísticas do batch atual). Se não chamarmos esse modo, o modelo não treinaria da forma correta, porque essas camadas se comportam de outro jeito quando estamos avaliando ou inferindo.

### b. optimizer.step()

É o comando que realmente atualiza os pesos e bias do modelo com base nos gradientes que foram calculados. Ou seja, é aqui que acontece o “aprendizado”: os parâmetros são ajustados na direção que diminui a função de perda. Sem essa linha, o modelo ficaria parado, só calculando gradientes, mas sem nunca se ajustar.

### c. Diferença fundamental entre model.train() e model.eval()

O model.train() serve para treino, enquanto o model.eval() serve para avaliação ou inferência. A diferença prática está nas camadas que mudam de comportamento:

- No train(), o dropout está ativo (neurônios desligados aleatoriamente) e a batch normalization usa estatísticas do batch atual.

- No eval(), o dropout é desativado (todos neurônios ativos) e a batch normalization usa estatísticas já aprendidas.

Além disso, durante avaliação/inferência é comum usar torch.no_grad() para economizar memória e tempo, já que não precisamos calcular gradientes. Essa diferença é crucial porque garante que o modelo se comporte de forma estável e reprodutível quando está apenas sendo usado, e de forma não fixa e regularizada quando está aprendendo.