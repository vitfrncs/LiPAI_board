{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMPsouSCZTn3RUaN+6fBs4T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vitfrncs/LiPAI_board/blob/master/src/semana_9/Semana9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Semana 9 - Data augmentation"
      ],
      "metadata": {
        "id": "BcrBCf58ptAq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "1 - Modifique a etapa de transformações (trans) para incluir uma rotação aleatória nas imagens de treinamento. Utilize torchvision.transforms.RandomRotation com um ângulo de até 15 graus. Como ficaria a nova definição de transforms.Compose?\n",
        "\n",
        "A etapa de transformações foi modificada para incluir uma rotação aleatória de até 15° nas imagens do conjunto de treinamento, utilizando torchvision.transforms.RandomRotation(15), enquanto o conjunto de teste permaneceu apenas com ToTensor e Normalize, garantindo que a rotação seja aplicada apenas durante o treinamento como técnica de data augmentation.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "# MNIST dataset\n",
        "root_path = '/home/storopoli/Downloads' # mude isso no Colab se necessário\n",
        "\n",
        "#QUESTAO 1:\n",
        "# Pequena transformação para tensores e normalizando o tamanho\n",
        "#trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "# transformações para o conjunto de treinamento\n",
        "train_trans = transforms.Compose([\n",
        "    transforms.RandomRotation(15),                      # rotação aleatória até ±15°\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# transformações para o conjunto de teste (sem rotação!)\n",
        "test_trans = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "\n",
        "# Train/Test Datasets\n",
        "# como estava antes:\n",
        "# train_dataset = torchvision.datasets.MNIST(root=root_path, train=True, transform=trans, download=True)\n",
        "# test_dataset = torchvision.datasets.MNIST(root=root_path, train=False, transform=trans)\n",
        "\n",
        "# como ficou:\n",
        "train_dataset = torchvision.datasets.MNIST(root=root_path, train=True, transform=train_trans, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root=root_path, train=False, transform=test_trans)\n"
      ],
      "metadata": {
        "id": "eTHKHtSrrNpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "2 - Construa uma nova estratégia de transformação chamada trans_aumentado que aplique sequencialmente as seguintes técnicas de aumento de dados:\n",
        "- Translação horizontal e vertical aleatória de até 10% da dimensão da imagem (RandomAffine).\n",
        "- Zoom aleatório, variando a escala da imagem entre 90% e 110% (RandomAffine).\n",
        "- Alteração de perspectiva (RandomPerspective).\n",
        "\n",
        "Após criar essa estratégia, substitua a etapa de transformação original no train_dataset e treine o modelo novamente."
      ],
      "metadata": {
        "id": "SKBRXorvs5hA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BzwNJJswppKC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "# MNIST dataset\n",
        "root_path = '/home/storopoli/Downloads' # mude isso no Colab se necessário\n",
        "\n",
        "# Pequena transformação para tensores e normalizando o tamanho\n",
        "#trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "#QUESTAO 2:\n",
        "trans_aumentado = transforms.Compose([\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),   # translação até 10%\n",
        "    transforms.RandomAffine(degrees=0, scale=(0.9, 1.1)),       # zoom 90% a 110%\n",
        "    transforms.RandomPerspective(distortion_scale=0.5, p=0.5),  # alteração de perspectiva\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "test_trans = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Train/Test Datasets\n",
        "# como estava antes:\n",
        "# train_dataset = torchvision.datasets.MNIST(root=root_path, train=True, transform=trans, download=True)\n",
        "# test_dataset = torchvision.datasets.MNIST(root=root_path, train=False, transform=trans)\n",
        "\n",
        "# como ficou:\n",
        "train_dataset = torchvision.datasets.MNIST(root=root_path, train=True, transform=trans_aumentado, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root=root_path, train=False, transform=test_trans)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "3 - A transformação transforms.RandomHorizontalFlip() é muito comum em problemas de visão computacional. Você a aplicaria para o dataset MNIST? Justifique sua resposta pensando em como essa transformação afetaria a classificação de dígitos específicos (por exemplo, o '6' e o '9').\n",
        "\n",
        "Não seria adequado aplicar transforms.RandomHorizontalFlip() no dataset MNIST, pois a inversão horizontal pode alterar completamente o significado de alguns dígitos, como transformar um “6” em algo semelhante a um “9” ou distorcer o “2” e o “5”, o que introduziria ruído nos dados em vez de melhorar a capacidade de generalização do modelo."
      ],
      "metadata": {
        "id": "CxxxZyKOuWxK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "T-TRC0v2N5o6"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size=32\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ggcjMGafN5o7"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8HuH9pypN5o7",
        "outputId": "92c2c971-502b-48f9-81ad-729722170f35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConvNet(\n",
            "  (layer1): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc1): Sequential(\n",
            "    (0): Linear(in_features=3136, out_features=1000, bias=True)\n",
            "    (1): ReLU()\n",
            "  )\n",
            "  (fc2): Linear(in_features=1000, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(7 * 7 * 64, 1000),\n",
        "            nn.ReLU())\n",
        "        self.fc2 = nn.Linear(1000, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# Instancia o Model()\n",
        "model = ConvNet()\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QaQB42AEN5o7",
        "outputId": "8908239e-2713-4707-def5-44ac609b074a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3199106"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "count_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GR5TgpiEN5o7"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "# Hiperparâmetros\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.001\n",
        "epochs = 6\n",
        "\n",
        "# Instânciar o Otimizador Adam\n",
        "optimizer = Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "p6HS0MmbN5o8",
        "outputId": "2636a2b2-75b8-4fba-c6ee-9aa09264d962",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Isto tem que retornar True\n",
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UH1_e6q-N5o8",
        "outputId": "28fcc9c3-cd6c-4fb9-d7cd-dacfc92be4ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla T4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Sua GPU\n",
        "torch.cuda.get_device_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dMYzYpqdN5o8",
        "outputId": "927e8060-d99d-4a92-fb5c-767de2b59c7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época [1/6], Step [100/1875], Custo: 0.423, Acurácia: 87.5\n",
            "Época [1/6], Step [200/1875], Custo: 0.404, Acurácia: 84.375\n",
            "Época [1/6], Step [300/1875], Custo: 0.309, Acurácia: 84.375\n",
            "Época [1/6], Step [400/1875], Custo: 0.146, Acurácia: 96.875\n",
            "Época [1/6], Step [500/1875], Custo: 0.154, Acurácia: 93.75\n",
            "Época [1/6], Step [600/1875], Custo: 0.24, Acurácia: 93.75\n",
            "Época [1/6], Step [700/1875], Custo: 0.27, Acurácia: 90.625\n",
            "Época [1/6], Step [800/1875], Custo: 0.108, Acurácia: 96.875\n",
            "Época [1/6], Step [900/1875], Custo: 0.037, Acurácia: 100.0\n",
            "Época [1/6], Step [1000/1875], Custo: 0.07, Acurácia: 96.875\n",
            "Época [1/6], Step [1100/1875], Custo: 0.2, Acurácia: 93.75\n",
            "Época [1/6], Step [1200/1875], Custo: 0.066, Acurácia: 96.875\n",
            "Época [1/6], Step [1300/1875], Custo: 0.171, Acurácia: 90.625\n",
            "Época [1/6], Step [1400/1875], Custo: 0.096, Acurácia: 96.875\n",
            "Época [1/6], Step [1500/1875], Custo: 0.508, Acurácia: 93.75\n",
            "Época [1/6], Step [1600/1875], Custo: 0.076, Acurácia: 96.875\n",
            "Época [1/6], Step [1700/1875], Custo: 0.133, Acurácia: 93.75\n",
            "Época [1/6], Step [1800/1875], Custo: 0.035, Acurácia: 96.875\n",
            "Época [2/6], Step [100/1875], Custo: 0.177, Acurácia: 93.75\n",
            "Época [2/6], Step [200/1875], Custo: 0.04, Acurácia: 100.0\n",
            "Época [2/6], Step [300/1875], Custo: 0.105, Acurácia: 96.875\n",
            "Época [2/6], Step [400/1875], Custo: 0.022, Acurácia: 100.0\n",
            "Época [2/6], Step [500/1875], Custo: 0.175, Acurácia: 93.75\n",
            "Época [2/6], Step [600/1875], Custo: 0.383, Acurácia: 96.875\n",
            "Época [2/6], Step [700/1875], Custo: 0.158, Acurácia: 93.75\n",
            "Época [2/6], Step [800/1875], Custo: 0.056, Acurácia: 100.0\n",
            "Época [2/6], Step [900/1875], Custo: 0.041, Acurácia: 96.875\n",
            "Época [2/6], Step [1000/1875], Custo: 0.028, Acurácia: 100.0\n",
            "Época [2/6], Step [1100/1875], Custo: 0.216, Acurácia: 93.75\n",
            "Época [2/6], Step [1200/1875], Custo: 0.003, Acurácia: 100.0\n",
            "Época [2/6], Step [1300/1875], Custo: 0.11, Acurácia: 93.75\n",
            "Época [2/6], Step [1400/1875], Custo: 0.008, Acurácia: 100.0\n",
            "Época [2/6], Step [1500/1875], Custo: 0.02, Acurácia: 100.0\n",
            "Época [2/6], Step [1600/1875], Custo: 0.045, Acurácia: 96.875\n",
            "Época [2/6], Step [1700/1875], Custo: 0.006, Acurácia: 100.0\n",
            "Época [2/6], Step [1800/1875], Custo: 0.387, Acurácia: 90.625\n",
            "Época [3/6], Step [100/1875], Custo: 0.134, Acurácia: 96.875\n",
            "Época [3/6], Step [200/1875], Custo: 0.025, Acurácia: 100.0\n",
            "Época [3/6], Step [300/1875], Custo: 0.178, Acurácia: 93.75\n",
            "Época [3/6], Step [400/1875], Custo: 0.046, Acurácia: 96.875\n",
            "Época [3/6], Step [500/1875], Custo: 0.086, Acurácia: 96.875\n",
            "Época [3/6], Step [600/1875], Custo: 0.033, Acurácia: 100.0\n",
            "Época [3/6], Step [700/1875], Custo: 0.06, Acurácia: 96.875\n",
            "Época [3/6], Step [800/1875], Custo: 0.041, Acurácia: 96.875\n",
            "Época [3/6], Step [900/1875], Custo: 0.019, Acurácia: 100.0\n",
            "Época [3/6], Step [1000/1875], Custo: 0.044, Acurácia: 96.875\n",
            "Época [3/6], Step [1100/1875], Custo: 0.035, Acurácia: 100.0\n",
            "Época [3/6], Step [1200/1875], Custo: 0.086, Acurácia: 96.875\n",
            "Época [3/6], Step [1300/1875], Custo: 0.253, Acurácia: 90.625\n",
            "Época [3/6], Step [1400/1875], Custo: 0.079, Acurácia: 93.75\n",
            "Época [3/6], Step [1500/1875], Custo: 0.017, Acurácia: 100.0\n",
            "Época [3/6], Step [1600/1875], Custo: 0.106, Acurácia: 96.875\n",
            "Época [3/6], Step [1700/1875], Custo: 0.008, Acurácia: 100.0\n",
            "Época [3/6], Step [1800/1875], Custo: 0.051, Acurácia: 100.0\n",
            "Época [4/6], Step [100/1875], Custo: 0.201, Acurácia: 93.75\n",
            "Época [4/6], Step [200/1875], Custo: 0.001, Acurácia: 100.0\n",
            "Época [4/6], Step [300/1875], Custo: 0.075, Acurácia: 93.75\n",
            "Época [4/6], Step [400/1875], Custo: 0.023, Acurácia: 100.0\n",
            "Época [4/6], Step [500/1875], Custo: 0.029, Acurácia: 100.0\n",
            "Época [4/6], Step [600/1875], Custo: 0.026, Acurácia: 100.0\n",
            "Época [4/6], Step [700/1875], Custo: 0.147, Acurácia: 96.875\n",
            "Época [4/6], Step [800/1875], Custo: 0.214, Acurácia: 93.75\n",
            "Época [4/6], Step [900/1875], Custo: 0.249, Acurácia: 93.75\n",
            "Época [4/6], Step [1000/1875], Custo: 0.001, Acurácia: 100.0\n",
            "Época [4/6], Step [1100/1875], Custo: 0.011, Acurácia: 100.0\n",
            "Época [4/6], Step [1200/1875], Custo: 0.052, Acurácia: 96.875\n",
            "Época [4/6], Step [1300/1875], Custo: 0.108, Acurácia: 93.75\n",
            "Época [4/6], Step [1400/1875], Custo: 0.004, Acurácia: 100.0\n",
            "Época [4/6], Step [1500/1875], Custo: 0.056, Acurácia: 96.875\n",
            "Época [4/6], Step [1600/1875], Custo: 0.002, Acurácia: 100.0\n",
            "Época [4/6], Step [1700/1875], Custo: 0.048, Acurácia: 96.875\n",
            "Época [4/6], Step [1800/1875], Custo: 0.003, Acurácia: 100.0\n",
            "Época [5/6], Step [100/1875], Custo: 0.086, Acurácia: 96.875\n",
            "Época [5/6], Step [200/1875], Custo: 0.027, Acurácia: 100.0\n",
            "Época [5/6], Step [300/1875], Custo: 0.032, Acurácia: 100.0\n",
            "Época [5/6], Step [400/1875], Custo: 0.071, Acurácia: 96.875\n",
            "Época [5/6], Step [500/1875], Custo: 0.109, Acurácia: 96.875\n",
            "Época [5/6], Step [600/1875], Custo: 0.01, Acurácia: 100.0\n",
            "Época [5/6], Step [700/1875], Custo: 0.007, Acurácia: 100.0\n",
            "Época [5/6], Step [800/1875], Custo: 0.083, Acurácia: 96.875\n",
            "Época [5/6], Step [900/1875], Custo: 0.069, Acurácia: 96.875\n",
            "Época [5/6], Step [1000/1875], Custo: 0.083, Acurácia: 96.875\n",
            "Época [5/6], Step [1100/1875], Custo: 0.042, Acurácia: 96.875\n",
            "Época [5/6], Step [1200/1875], Custo: 0.033, Acurácia: 100.0\n",
            "Época [5/6], Step [1300/1875], Custo: 0.113, Acurácia: 93.75\n",
            "Época [5/6], Step [1400/1875], Custo: 0.033, Acurácia: 100.0\n",
            "Época [5/6], Step [1500/1875], Custo: 0.007, Acurácia: 100.0\n",
            "Época [5/6], Step [1600/1875], Custo: 0.396, Acurácia: 90.625\n",
            "Época [5/6], Step [1700/1875], Custo: 0.041, Acurácia: 96.875\n",
            "Época [5/6], Step [1800/1875], Custo: 0.003, Acurácia: 100.0\n",
            "Época [6/6], Step [100/1875], Custo: 0.047, Acurácia: 96.875\n",
            "Época [6/6], Step [200/1875], Custo: 0.093, Acurácia: 96.875\n",
            "Época [6/6], Step [300/1875], Custo: 0.051, Acurácia: 96.875\n",
            "Época [6/6], Step [400/1875], Custo: 0.004, Acurácia: 100.0\n",
            "Época [6/6], Step [500/1875], Custo: 0.046, Acurácia: 96.875\n",
            "Época [6/6], Step [600/1875], Custo: 0.004, Acurácia: 100.0\n",
            "Época [6/6], Step [700/1875], Custo: 0.239, Acurácia: 93.75\n",
            "Época [6/6], Step [800/1875], Custo: 0.179, Acurácia: 90.625\n",
            "Época [6/6], Step [900/1875], Custo: 0.004, Acurácia: 100.0\n",
            "Época [6/6], Step [1000/1875], Custo: 0.301, Acurácia: 93.75\n",
            "Época [6/6], Step [1100/1875], Custo: 0.026, Acurácia: 96.875\n",
            "Época [6/6], Step [1200/1875], Custo: 0.041, Acurácia: 96.875\n",
            "Época [6/6], Step [1300/1875], Custo: 0.003, Acurácia: 100.0\n",
            "Época [6/6], Step [1400/1875], Custo: 0.002, Acurácia: 100.0\n",
            "Época [6/6], Step [1500/1875], Custo: 0.01, Acurácia: 100.0\n",
            "Época [6/6], Step [1600/1875], Custo: 0.0, Acurácia: 100.0\n",
            "Época [6/6], Step [1700/1875], Custo: 0.082, Acurácia: 93.75\n",
            "Época [6/6], Step [1800/1875], Custo: 0.002, Acurácia: 100.0\n"
          ]
        }
      ],
      "source": [
        "# Treinar o Modelo\n",
        "total_step = len(train_loader) # quantos batches eu tenho\n",
        "\n",
        "# Listas vazias\n",
        "loss_list = []\n",
        "acc_list = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Gera a propagação (feed forward)\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Calcula a função-custo\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss_list.append(loss.item())\n",
        "\n",
        "        # Retro-propagação (Backprop) e a otimização com Adam\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Acurácia\n",
        "        total = labels.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        correct = (predicted == labels).sum().item()\n",
        "        acc_list.append(correct / total)\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"Época [{epoch+1}/{epochs}], Step [{i+1}/{total_step}], Custo: {round(loss.item(), 3)}, Acurácia: {round((correct / total) * 100, 3)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "H-u3Csbzqa77",
        "outputId": "fe90613d-c4db-4aea-bcfb-28be613b6b6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia do Modelo em 10k imagens de teste: 99.19\n"
          ]
        }
      ],
      "source": [
        "model.eval() # coloca o modelo em modo de avaliação (sem calcular gradientes)\n",
        "\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        # Feed-forward com as imagens de teste\n",
        "        outputs = model(images)\n",
        "\n",
        "        # gera predições usando a função max()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Acumula total e corretas\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Acurácia do Modelo em 10k imagens de teste: {round((correct / total) * 100, 3)}\")"
      ]
    }
  ]
}