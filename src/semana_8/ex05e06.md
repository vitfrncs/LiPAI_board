
---
#### **QUESTÃO 5:** Vamos explorar modificações na arquitetura da rede ou no processo de treinamento para tentar melhorar ainda mais esse resultado. Explore diferentes otimizadores e taxa de aprendizado no modelo. Descreve o que foi observado com essas atualizações em relação ao processo de treinamento, validação e teste. Os modelos ficaram melhores?

Com valores originais:


    Época 1: acc: 96.70, loss: 0.5261
    Novo melhor modelo salvo com acc=96.70%
    Época 2: acc: 98.58, loss: 0.2311
    Novo melhor modelo salvo com acc=98.58%
    Época 3: acc: 98.98, loss: 0.1736
    Novo melhor modelo salvo com acc=98.98%
    Época 4: acc: 99.16, loss: 0.1387
    Novo melhor modelo salvo com acc=99.16%
    Época 5: acc: 99.25, loss: 0.1221
    Novo melhor modelo salvo com acc=99.25%
    Época 6: acc: 99.41, loss: 0.1131
    Novo melhor modelo salvo com acc=99.41%



Adam com lr=0.0001 (menor)

    Época 1: acc: 94.79, loss: 0.9155
    Novo melhor modelo salvo com acc=94.79%
    Época 2: acc: 98.18, loss: 0.2965
    Novo melhor modelo salvo com acc=98.18%
    Época 3: acc: 98.73, loss: 0.2037
    Novo melhor modelo salvo com acc=98.73%
    Época 4: acc: 99.05, loss: 0.1586
    Novo melhor modelo salvo com acc=99.05%
    Época 5: acc: 99.19, loss: 0.1297
    Novo melhor modelo salvo com acc=99.19%
    Época 6: acc: 99.31, loss: 0.1130
    Novo melhor modelo salvo com acc=99.31%

- Acurácia final: 99.31% (ligeiramente menor que a original).

- Convergência mais lenta nas primeiras épocas (Época 1: 94.79% vs 96.70% no original).

- Perda inicial mais alta (0.9155 vs 0.5261), mas estabilizou e convergiu no final.

- Treinamento mais suave, mas demorou mais para atingir o mesmo desempenho.

Adam com lr=0.002 (maior)

    Época 1: acc: 96.05, loss: 0.6312
    Novo melhor modelo salvo com acc=96.05%
    Época 2: acc: 98.22, loss: 0.2905
    Novo melhor modelo salvo com acc=98.22%
    Época 3: acc: 98.66, loss: 0.2274
    Novo melhor modelo salvo com acc=98.66%
    Época 4: acc: 98.62, loss: 0.2341
    Época 5: acc: 99.03, loss: 0.1812
    Novo melhor modelo salvo com acc=99.03%
    Época 6: acc: 98.78, loss: 0.2312

- Taxa de aprendizado maior faz o modelo aprender mais rápido, mas pode causar flutuações na acurácia e na perda, especialmente em épocas intermediárias.

- O modelo teve desempenho menos estável, com pequenas quedas na validação, mostrando risco de “overstepping” nos pesos.

SGD (Stochastic Gradient Descent) com momentum=0.9 e lr = 0.01

    Época 1: acc: 96.10, loss: 0.6143
    Novo melhor modelo salvo com acc=96.10%
    Época 2: acc: 98.73, loss: 0.2091
    Novo melhor modelo salvo com acc=98.73%
    Época 3: acc: 99.14, loss: 0.1362
    Novo melhor modelo salvo com acc=99.14%
    Época 4: acc: 99.34, loss: 0.1118
    Novo melhor modelo salvo com acc=99.34%
    Época 5: acc: 99.52, loss: 0.0825
    Novo melhor modelo salvo com acc=99.52%
    Época 6: acc: 99.56, loss: 0.0763
    Novo melhor modelo salvo com acc=99.56%

-vAcurácia final: 99.56% → ligeiramente melhor que Adam 0.001 (99.41%).

- Começa semelhante ao Adam (Época 1: 96.10%), mas a perda diminui mais rápido nas primeiras épocas.

- Treinamento e validação estão estáveis, sem quedas perceptíveis.

- O momentum 0.9 ajuda a suavizar as atualizações, acelerando a convergência sem grandes flutuações. Isso permite que o SGD alcance acurácia final ligeiramente superior ao Adam neste caso.

---
#### **QUESTÃO 6:** Qual a diferença entre os modos model.train() e model.eval()? Por que é crucial usar model.eval() e com torch.no_grad() durante a fase de teste/avaliação?

model.train() coloca o modelo em modo treino, ativando Dropout e usando médias do batch no BatchNorm, essenciais para atualizar pesos e aplicar regularização. model.eval() coloca o modelo em modo avaliação, desativa o Dropout e usa estatísticas fixas do BatchNorm, garantindo saídas consistentes. Usar torch.no_grad() durante avaliação economiza memória e acelera a execução, já que não há cálculo de gradientes.